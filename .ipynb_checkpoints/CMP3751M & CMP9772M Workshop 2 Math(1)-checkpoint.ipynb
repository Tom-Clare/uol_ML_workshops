{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning 2021 - 2022 <br> \n",
    "Week 2 - Workshop on Maths <br>\n",
    "Lecturer: Bashir Al-Diri <br>\n",
    "\n",
    "\n",
    "# Machine Learning Workshop: Week 2\n",
    "\n",
    "This week we will use our math knowledge to make some basic preparations to understand linear regression (next lecture). We will have a look at vectors, matrices, error functions, derivatives and finding the minimum of a function. \n",
    "\n",
    "\n",
    "Please read over the whole notebook. It contains several excercises (6) that you have to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors and Matrices in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a vector by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,0,2])\n",
    "b = np.array([3,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector only has one dimension in python (its not directly treated as matrix). We can see that as the second dimension (calling shape) is left free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add vectors and compute the inner product with the dot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + b\n",
    "d = a.dot(c)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix is created by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3), (3, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[2, 1, 3], [1, 1 ,2]])\n",
    "B = np.array([[2, 1], [1, 2], [5 ,2]])\n",
    "\n",
    "A.shape, B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have created a $2x3$ and a $3x2$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or vectors can be stacked into matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 3],\n",
       "        [0, 2],\n",
       "        [2, 1]]), array([[1, 0, 2],\n",
       "        [3, 2, 1]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.column_stack((a,b))\n",
    "Y = np.row_stack((a,b))\n",
    "\n",
    "X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add, transpose and multiply matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4, 2, 8],\n",
       "        [2, 3, 4]]), array([[34, 22],\n",
       "        [19, 13]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = A + B.transpose()\n",
    "D = C.dot(A.transpose()) # matrix product C * A\n",
    "C,D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can multiply matrices with vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8, 5]), array([12,  5]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = A.dot(a) # this corresponds to A * a\n",
    "f = a.dot(B) # this corresponds to a^T * B\n",
    "e, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse of a matrix can be computed by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[14,  9],\n",
       "        [ 9,  6]]), array([[ 2.        , -3.        ],\n",
       "        [-3.        ,  4.66666667]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy.linalg as linalg\n",
    "\n",
    "AA = A.dot(A.transpose()) # A * A^T ... we can only invert quadratic matrices\n",
    "AAinv = linalg.inv(AA)\n",
    "\n",
    "AA, AAinv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying with the inverse needs to result in the Identity matrix (from both sides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0.],\n",
       "        [0., 1.]]), array([[1., 0.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AA.dot(AAinv), AAinv.dot(AA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Computing the inverse of a matrix is tricky and it is hard to get a numerically accurate solution. Whenever we need to compute the inverse of a matrix times another matrix ($\\boldsymbol{A}^{-1}\\boldsymbol{B}$, then it is better to use specifically tailored methods for this which are numerically more stable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.        ,  6.33333333])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy.linalg as linalg\n",
    "# compute A^-1*b in a more stable way using linalg.solve.\n",
    "b = np.array([1, 2])\n",
    "out1 = linalg.solve(AA, b)\n",
    "\n",
    "out1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "Compute: \n",
    "* $(\\boldsymbol{A} \\boldsymbol a - \\boldsymbol b)^T(\\boldsymbol A \\boldsymbol a - \\boldsymbol b)$, \n",
    "* $(\\boldsymbol{C} \\boldsymbol b)^T\\boldsymbol C$\n",
    "* $(\\boldsymbol{C}^T \\boldsymbol C)^{-1}\\boldsymbol C^T \\boldsymbol a$, \n",
    "\n",
    "where the matrices are defined below. Check your result also in terms of the dimensionalities of the resulting matrices. Thats an easy way of spotting an error. Always use the linalg.solve method instead of the linalg.inv method if possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, array([34, 90]), array([1.00000000e+00, 5.55111512e-17]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 0, 1], [2, 3, 1]])\n",
    "C = np.array([[1, 0], [2, 3], [1, 5]])\n",
    "a = np.array([1,2,1])\n",
    "b = np.array([2,2])\n",
    "\n",
    "p1 = (A.dot(a)-b).transpose()\n",
    "p2 = A.dot(a) - b\n",
    "sol1 = p1.dot(p2)\n",
    "\n",
    "\n",
    "sol2 = C.dot(b).transpose().dot(C)\n",
    "\n",
    "\n",
    "sol3 = linalg.inv(C.transpose().dot(C)).dot(C.transpose()).dot(a)\n",
    "\n",
    "sol1, sol2, sol3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Matrix Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/regression.png'>\n",
    "We want to find a linear function (line) that best fits the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Functions in vector form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, we want to fit a linear function of the following form:\n",
    "    $$\\hat y = \\beta_0 + \\sum_i \\beta_i x_i $$\n",
    "$\\beta_0$ is the offset and $\\beta_i$ defines the slope for the ith input.\n",
    "We can also write the output $\\hat{y}$ in vector form\n",
    "$$\\hat{y} = \\beta_0  + \\boldsymbol x^T\\boldsymbol \\beta = \\tilde{\\boldsymbol x}^T \\tilde{\\boldsymbol \\beta}, \\textrm{ with } \\tilde{\\boldsymbol \\beta} = \\left[\\begin{array}{c}\\beta_0 \\\\ \\vdots \\\\ \\beta_d \\end{array} \\right] \\textrm{ and } \\tilde{\\boldsymbol x} = \\left[\\begin{array}{c}1 \\\\ x_1 \\\\ \\vdots \\\\ x_d \\end{array} \\right]$$\n",
    "Note that we prepended a one to the $\\boldsymbol x$-vector which will multiply with the offset $\\beta_0$ when computing the scalar product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices for multiple samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now consider multiple samples $\\boldsymbol x_i$, where we will prepend again a $1$ to create the $\\tilde{\\boldsymbol x}_{i} = \\left[\\begin{array}{c}1 \\\\ x_{i,1} \\\\ \\vdots \\\\ x_{i,d} \\end{array} \\right]$ \n",
    " vector. We can stack all $\\tilde{\\boldsymbol x_{i}}$  in a matrix $\\tilde{\\boldsymbol X} = \\left[\\begin{array}{c}\\tilde{\\boldsymbol x}_{d}\\\\ \\vdots \\\\ \\tilde{\\boldsymbol x}_{n}  \\end{array} \\right].$\n",
    "The output $\\hat y_i$ for each sample can also be  subsumed in a vector \n",
    "$\\hat{\\boldsymbol y} = \\left[\\begin{array}{c}\\hat{y}_{1}\\\\ \\vdots \\\\ \\hat{ y}_{n}  \\end{array} \\right] = \\left[\\begin{array}{c} \\tilde{\\boldsymbol x_1}^T \\tilde{\\boldsymbol \\beta} \\\\ \\vdots \\\\ \\tilde{\\boldsymbol x_n}^T \\tilde{\\boldsymbol \\beta}  \\end{array} \\right] = \\tilde{\\boldsymbol X} \\tilde{\\boldsymbol{\\beta}}.$ Hence, the computation of all output values can be written as matrix vector product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets do it in python..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a 1-dimensional problem as illustrated below. We are given 10 training samples and we want to fit\n",
    "a line to these samples. Our line has 2 parameters, $\\beta_0$ and $\\beta_1$. Lets first look at the data and how we can compute a prediction using hand-picked \n",
    "$\\beta_0$ and $\\beta_1$ values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot\n",
    "\n",
    "data_train = pd.read_csv('regression_train.csv')\n",
    "data_test = pd.read_csv('regression_test.csv')\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the training data as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = data_train['x']\n",
    "y_train = data_train['y']\n",
    "\n",
    "x_test = data_test['x']\n",
    "y_test = data_test['y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "## get input output vectors from the data frame and plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_train,y_train, 'bo')\n",
    "plt.savefig('trainingdata.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data matrix\n",
    "As a first step, lets construct the $\\tilde{\\boldsymbol{X}}$ matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xtilde = np.column_stack((np.ones(x_train.shape), x_train))\n",
    "Xtilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Prediction with arbitrary betas.\n",
    "We want to extend the plot now with the prediction for a hand-picked betas. \n",
    "* First, compute the data matrix Xtest_tilde for the test set (see code above how it is done for the training set).\n",
    "* Reuse the plotting code from above, add the predicted line for $\\beta_0 = 7$ and $\\beta_1 = 8$ to the plot (use red line color, extend your legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta_0 = 7\n",
    "beta_1 = 6\n",
    "betatilde = np.array([beta_0, beta_1])\n",
    "\n",
    "Xtest_tilde = # put your code here...\n",
    "ytest_hat = # put your code here...\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_test,ytest_hat, 'r')\n",
    "plt.plot(x_train,y_train, 'bo')\n",
    "plt.plot(x_test,y_test, 'g')\n",
    "plt.legend(('predictions', 'training points', 'ground truth'), loc = 'lower right')\n",
    "\n",
    "plt.savefig('regression_randomPrediction.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error computation\n",
    "Well, not too bad but still a large error. We can do better. To assess the quality of our prediction, we compute the error as the difference to the training labels, i.e.,\n",
    "$$\\boldsymbol e = \\left[\\begin{array}{c}y_1 \\\\ \\vdots \\\\ y_n  \\end{array} \\right] - \\left[\\begin{array}{c}\\hat y_1 \\\\ \\vdots \\\\ \\hat y_n  \\end{array} \\right].$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat = Xtilde.dot(betatilde)\n",
    "error = y_train - yhat\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summed squared error can be computed by the scalar product of the error vector with itself, i.e.,\n",
    "$$SSE = \\boldsymbol{e}^T \\boldsymbol{e} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SSE = error.dot(error) # The scalar product is also implemented with the dot function (no need for transpose)\n",
    "SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - The SSE error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SSE is a function of the $\\tilde{\\boldsymbol{\\beta}}$ vector. We want to minimize the SSE, i.e., find the  \n",
    "$\\tilde{\\boldsymbol{\\beta}}$ with minimal summed squared error. In this excercise, you should visualize the error function by testing different $\\beta_0$ and $\\beta_1$ values and creating a 3D plot.\n",
    "\n",
    "First, implement a function SSE that takes the betaTilde vector, the training input x and the training output y and computes the sum of squarred errors for the given betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SSE(beta, x, y):\n",
    "    \n",
    "    error = # Put your code here..\n",
    "    SSE = # Put your code here.. \n",
    "    return SSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the SSE for a grid from $-200$ to $200$, using $50$ grid values per dimension. I.e., in total we get \n",
    "$2500$ evaluations of the SSE function. If your SSE function has ben executed without errors correctly, you just have to execute the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# specify data points for beta0 and beta1 (from - 200 to 200, using 50 uniformly distributed points)\n",
    "beta0Array = np.linspace(-200, 200, 50)\n",
    "beta1Array = np.linspace(-200, 200, 50)\n",
    "\n",
    "SSEarray = np.zeros((50,50))\n",
    "\n",
    "for i in range(0,50):\n",
    "    for j in range(0,50):\n",
    "        beta = np.array([beta0Array[i], beta1Array[j]])\n",
    "        SSEarray[i,j] =  SSE(beta, x_train, y_train)\n",
    "        \n",
    "SSEarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell ouputs the SSE for every grid position between -200 and 200 for both dimensions. For a better visualization, we can create a 3D plot. Run the following cell for doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "beta0Grid, beta1Grid = np.meshgrid(beta0Array, beta1Array)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(beta0Grid, beta1Grid, SSEarray, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.xlabel('beta0')\n",
    "plt.ylabel('beta1')\n",
    "plt.savefig('errorfunction.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization of the SSE\n",
    "We have seen that we need to minimize a quadratic function that depends on 2 or more parameters ($\\beta_0$ to $\\beta_d$). We could now take the best $\\boldsymbol \\beta$ values that you have seen on the grid. However, this is highly inefficient for high-dimensional problems and also inaccurate. In order to do this minimization properly, we first have to talk about derivatives and vector derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the optimal beta values on our grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minIndex = np.argmin(SSEarray)\n",
    "index1, index2 = np.unravel_index(minIndex, (50,50))\n",
    "\n",
    "beta0 = beta0Array[index1]\n",
    "beta1 = beta1Array[index2]\n",
    "\n",
    "beta0, beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Plot the line\n",
    "Using the found beta values from before, plot the resulting line in a similar plot as for excercise 2 and compute the SSE. Is it better then our hand-picked value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** “The derivative of a function of a real variable measures the sensitivity to change of a quantity (a function value or dependent variable) which is determined by another quantity (the independent variable)”. \n",
    "<img src=\"files/derivative.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivative Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to computer a derivative $\\partial f(x) / \\partial x $ of a function $f(x)$, we can use the following rules:\n",
    "* **Constants**:\n",
    "$$ \\frac{\\partial a}{\\partial x} = 0$$ \n",
    "* **Linear term:** \n",
    "$$ \\frac{\\partial ax}{\\partial x} = a$$\n",
    "* **Quadratic terms**:\n",
    "$$ \\frac{\\partial x^2}{\\partial x} = 2x$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivative Rules cont."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Linearity**:\n",
    "$$ \\frac{\\partial a\\cdot f(x) + b \\cdot g(x)}{\\partial x} = a\\frac{\\partial  f(x)}{\\partial x} + b \\frac{\\partial g(x)}{\\partial x}$$\n",
    "* **Chain Rule**: For a composition of functions, i.e., $y = f(g(x))$, we can introduce an auxiliary variable $u = g(x)$. The derivative of  $$y = f(g(x))$$ is then given by\n",
    "$$ \\frac{\\partial f(g(x)) }{\\partial x} = \\frac{\\partial  f(u)}{\\partial u} \\frac{\\partial u}{\\partial x},$$\n",
    "i.e. we have to multiply the derivative of $f$ with the deriviative of $u = g(x)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the derivative of $h(x) = 2 (x ^2 - 4)^2$. This function can be decomposed in $f(u) = 2 u^2$ and $u=g(x) = x^2 -4$, with $h(x) = f(g(x))$.\n",
    "\n",
    "* Compute derivative of $f$: $$ \\frac{\\partial  f(u)}{\\partial u} = \\frac{\\partial  (2 u^2)}{\\partial u}= 4 u.$$\n",
    "* Compute derivative of $u$: $$ \\frac{\\partial  u}{\\partial x} = \\frac{\\partial  (x^2 - 4)}{\\partial x} = 2x.$$\n",
    "* Final Result:\n",
    "$$ \\frac{\\partial f(g(x)) }{\\partial x} = \\frac{\\partial  f(u)}{\\partial u} \\frac{\\partial u}{\\partial x} = 4 u \\cdot 2 x = 8 \\underbrace{(x^2 - 4)}_u x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the function and its derivative\n",
    "def h(x):\n",
    "    return 2 *(x**2 - 4) ** 2\n",
    "\n",
    "def hd(x):\n",
    "    return 8*(x ** 2 - 4) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot it\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "x = np.linspace(-3, 3, num=100)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(x, h(x))\n",
    "plt.ylabel('h(x)')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(x, hd(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('hd(x)')\n",
    "\n",
    "plt.savefig('function_derivatives.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the function has 3 extrema (2 minima and 1 maximum), which occur when its derivative is 0 (-2, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Gradient Descent algoithm to find a minimum or a maximum of this function. Now write your code to find a minimum using Gradient Descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code here to find a minimum of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Derivatives\n",
    "\n",
    "Compute the derivative of $v(x) = (x  + 4)^2 + 3 x^2 - (x - 1)^2$. Plot the function and its derivative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the minima of a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that a function has an extremum of a function occurs when its derivative is 0. For certain types of functions, i.e.,\n",
    "quadratic functions, we can directly compute where the extrenum (i.e. max or minimum) is. Note that this only works if the function is convex,\n",
    "i.e., there is only one maximum or minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Example: ** We want to compute the minimum of the following function: $f(x) = 2 x^2 - 2x + 1$ \n",
    "* **Derivative:** \n",
    "$$\\frac{\\partial f(x)}{\\partial x} = 4x - 2 $$\n",
    "* **Compute minimum:**\n",
    "\\begin{align*} \\frac{\\partial f(x)}{\\partial x} = &0 \\\\\n",
    "4 x - 2 = &0 \\\\\n",
    "x  = & \\frac{1}{2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets confirm that with a plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, num=100)\n",
    "plt.figure()\n",
    "plt.plot(x, 2 * x ** 2 - 2 * x + 1)\n",
    "plt.savefig('function_minimum.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Computing the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the minimum of the function defined in excercise 5. Confirm your result in the plot from  excercise 5."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
